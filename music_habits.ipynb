{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:26:32.302029Z",
     "end_time": "2023-08-02T13:26:35.572749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.3.2 in ./venv/lib/python3.8/site-packages (3.3.2)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in ./venv/lib/python3.8/site-packages (from pyspark==3.3.2) (0.10.9.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:26:35.580671Z",
     "end_time": "2023-08-02T13:26:35.715910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/02 13:26:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x10c2f5730>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://172.16.27.63:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>SparkMusicHabits</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkMusicHabits\").config(\"spark.driver.memory\", \"6g\").getOrCreate()\n",
    "# I had to tune the driver (which is also the executor) memory to avoid OOM errors, since I ran on local machine\n",
    "\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:26:35.706518Z",
     "end_time": "2023-08-02T13:26:49.236335Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "tsv_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"artist_id\", StringType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"track_id\", StringType(), True),\n",
    "    StructField(\"track_name\", StringType(), True)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:26:49.245665Z",
     "end_time": "2023-08-02T13:26:49.279285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|    user_id|          timestamp|           artist_id|    artist_name|            track_id|          track_name|\n",
      "+-----------+-------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|user_000001|2009-05-05 02:08:57|f1b1cf71-bd35-4e9...|      Deep Dish|                null|Fuck Me Im Famous...|\n",
      "|user_000001|2009-05-04 16:54:10|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Composition 0919 ...|\n",
      "|user_000001|2009-05-04 16:52:04|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Mc2 (Live_2009_4_15)|\n",
      "|user_000001|2009-05-04 16:42:52|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Hibari (Live_2009...|\n",
      "|user_000001|2009-05-04 16:42:11|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Mc1 (Live_2009_4_15)|\n",
      "|user_000001|2009-05-04 16:38:31|a7f7df4a-77d8-4f1...|       坂本龍一|                null|To Stanford (Live...|\n",
      "|user_000001|2009-05-04 16:33:28|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Improvisation (Li...|\n",
      "|user_000001|2009-05-04 16:23:45|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Glacier (Live_200...|\n",
      "|user_000001|2009-05-04 16:19:22|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Parolibre (Live_2...|\n",
      "|user_000001|2009-05-04 16:13:38|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Bibo No Aozora (L...|\n",
      "|user_000001|2009-05-04 16:06:09|a7f7df4a-77d8-4f1...|       坂本龍一|f7c1f8f8-b935-45e...|The Last Emperor ...|\n",
      "|user_000001|2009-05-04 16:00:48|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Happyend (Live_20...|\n",
      "|user_000001|2009-05-04 15:55:34|a7f7df4a-77d8-4f1...|       坂本龍一|475d4e50-cebb-4cd...|Tibetan Dance (Ve...|\n",
      "|user_000001|2009-05-04 15:51:26|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Behind The Mask (...|\n",
      "|user_000001|2009-05-03 18:48:25|ba2f4f3b-0293-4bc...|     Underworld|dc394163-2b78-4b5...|Boy, Boy, Boy (Sw...|\n",
      "|user_000001|2009-05-03 18:37:56|ba2f4f3b-0293-4bc...|     Underworld|340d9a0b-9a43-409...|Crocodile (Innerv...|\n",
      "|user_000001|2009-05-03 18:14:53|a16e47f5-aa54-47f...|Ennio Morricone|0b04407b-f517-4e0...|Ninna Nanna In Bl...|\n",
      "|user_000001|2009-05-03 18:10:18|463a94f1-2713-40b...|        Minus 8|4e78efc4-e545-47a...|      Elysian Fields|\n",
      "|user_000001|2009-05-03 18:04:31|ad0811ea-e213-451...|      Beanfield|fb51d2c4-cc69-412...|  Planetary Deadlock|\n",
      "|user_000001|2009-05-03 17:56:25|309e2dfc-678e-4d0...|       Dj Linus|4277434f-e3c2-41a...|Good Morning Love...|\n",
      "|user_000001|2009-05-03 17:50:51|6f3d4a7b-45b2-4c0...|      Alif Tree|1151b040-8022-496...|      Deadly Species|\n",
      "|user_000001|2009-05-03 17:46:29|463a94f1-2713-40b...|        Minus 8|f78c95a8-9256-475...|         Cold Fusion|\n",
      "|user_000001|2009-05-03 17:39:20|45bdb5be-ec03-484...|        Wei-Chi|c4fc8802-d186-4c4...|              Clouds|\n",
      "|user_000001|2009-05-03 17:34:06|3c174d9d-139b-4fa...|      Marsmobil|7217acaf-0f12-4cc...|        Sovatex 2055|\n",
      "|user_000001|2009-05-03 17:28:02|f945fc79-fb9e-4e3...|          Karma|b79e44f0-2a27-4f5...|         Beach Towel|\n",
      "|user_000001|2009-05-03 17:20:15|ce559a88-58ba-4d8...|      Masomenos|                null|               Eight|\n",
      "|user_000001|2009-05-03 17:07:40|ce559a88-58ba-4d8...|      Masomenos|                null|               Seven|\n",
      "|user_000001|2009-05-03 17:01:15|ce559a88-58ba-4d8...|      Masomenos|                null|                 Six|\n",
      "|user_000001|2009-05-03 16:56:25|ce559a88-58ba-4d8...|      Masomenos|                null|                Five|\n",
      "|user_000001|2009-05-03 16:47:20|ce559a88-58ba-4d8...|      Masomenos|                null|                Four|\n",
      "+-----------+-------------------+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"userid-timestamp-artid-artname-traid-traname.tsv\",\n",
    "                     format=\"csv\", sep=\"\\t\", schema=tsv_schema, header=\"false\")\n",
    "df.show(30)  # Sanity check for our data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:26:49.292474Z",
     "end_time": "2023-08-02T13:26:54.243386Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+-----------+--------+----------+\n",
      "|user_id|timestamp|artist_id|artist_name|track_id|track_name|\n",
      "+-------+---------+---------+-----------+--------+----------+\n",
      "|      0|        0|   602166|          0| 2168588|         1|\n",
      "+-------+---------+---------+-----------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# fancy way to aggregate every col, and count the nulls, this is just EDA\n",
    "null_counts = df.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:26:54.244384Z",
     "end_time": "2023-08-02T13:27:43.551760Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ok so we can NOT count on artist_id or track_id, but the rest seems pretty good. Great!\n",
    "Note that fallback to rely on track_name only has drawbacks - say we have two songs with same title (which is quite common: https://www.thetoptens.com/music/different-songs-with-same-title/). More robust approach would be to rely on the artist_name & track_name combination"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "# let's create robust id, add new col: song_fullname\n",
    "df = df.withColumn('song_fullname', concat(col('artist_name'),col('track_name')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:27:43.569280Z",
     "end_time": "2023-08-02T13:27:43.679242Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+-----------+--------+--------------------+------------------------+\n",
      "|    user_id|          timestamp|           artist_id|artist_name|track_id|          track_name|           song_fullname|\n",
      "+-----------+-------------------+--------------------+-----------+--------+--------------------+------------------------+\n",
      "|user_000001|2009-05-05 02:08:57|f1b1cf71-bd35-4e9...|  Deep Dish|    null|Fuck Me Im Famous...|    Deep DishFuck Me ...|\n",
      "|user_000001|2009-05-04 16:54:10|a7f7df4a-77d8-4f1...|   坂本龍一|    null|Composition 0919 ...|坂本龍一Composition 0...|\n",
      "|user_000001|2009-05-04 16:52:04|a7f7df4a-77d8-4f1...|   坂本龍一|    null|Mc2 (Live_2009_4_15)|坂本龍一Mc2 (Live_200...|\n",
      "|user_000001|2009-05-04 16:42:52|a7f7df4a-77d8-4f1...|   坂本龍一|    null|Hibari (Live_2009...|坂本龍一Hibari (Live_...|\n",
      "|user_000001|2009-05-04 16:42:11|a7f7df4a-77d8-4f1...|   坂本龍一|    null|Mc1 (Live_2009_4_15)|坂本龍一Mc1 (Live_200...|\n",
      "|user_000001|2009-05-04 16:38:31|a7f7df4a-77d8-4f1...|   坂本龍一|    null|To Stanford (Live...|坂本龍一To Stanford (...|\n",
      "|user_000001|2009-05-04 16:33:28|a7f7df4a-77d8-4f1...|   坂本龍一|    null|Improvisation (Li...|坂本龍一Improvisation...|\n",
      "|user_000001|2009-05-04 16:23:45|a7f7df4a-77d8-4f1...|   坂本龍一|    null|Glacier (Live_200...|坂本龍一Glacier (Live...|\n",
      "|user_000001|2009-05-04 16:19:22|a7f7df4a-77d8-4f1...|   坂本龍一|    null|Parolibre (Live_2...|坂本龍一Parolibre (Li...|\n",
      "|user_000001|2009-05-04 16:13:38|a7f7df4a-77d8-4f1...|   坂本龍一|    null|Bibo No Aozora (L...|坂本龍一Bibo No Aozor...|\n",
      "+-----------+-------------------+--------------------+-----------+--------+--------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:27:43.640043Z",
     "end_time": "2023-08-02T13:27:43.990632Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part A: Create a list of user IDs, along with the number of distinct songs each user has played.\n",
    "\n",
    "# Approach - If it was pure SQL - than Select user id, count(DISTINCT(song_fullname)) Group by user id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|    user_id|distinct_songs_count|\n",
      "+-----------+--------------------+\n",
      "|user_000066|                 666|\n",
      "|user_000113|                2133|\n",
      "|user_000098|                 254|\n",
      "|user_000372|                4789|\n",
      "|user_000424|                2004|\n",
      "|user_000577|               18227|\n",
      "|user_000708|                4743|\n",
      "|user_000289|                 994|\n",
      "|user_000319|                6294|\n",
      "|user_000445|                3718|\n",
      "+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "distinct_songs_per_user = df.groupBy('user_id').agg(countDistinct('song_fullname').alias('distinct_songs_count'))\n",
    "\n",
    "distinct_songs_per_user.show(10)\n",
    "\n",
    "# you many want to write the df to a sink to get the full results, or to convert it to pandas df (if small enough) if you want to use it in python code\n",
    "distinct_songs_per_user.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"distinct_songs_user_played.tsv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:27:43.995883Z",
     "end_time": "2023-08-02T13:28:47.286951Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part B: Create a list of the 100 most popular songs (artist and title) in the dataset, with the number of times each was played.\n",
    "\n",
    "Approach: SQL solution would be like: Select artist_name, track_name, count(song_fullname) as times_played GROUP BY song_fullname order by times_played LIMIT 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------+\n",
      "|        artist_name|          track_name|times_played|\n",
      "+-------------------+--------------------+------------+\n",
      "| The Postal Service|  Such Great Heights|        3992|\n",
      "|       Boy Division|Love Will Tear Us...|        3663|\n",
      "|          Radiohead|        Karma Police|        3534|\n",
      "|               Muse|Supermassive Blac...|        3483|\n",
      "|Death Cab For Cutie|     Soul Meets Body|        3479|\n",
      "|          The Knife|          Heartbeats|        3156|\n",
      "|               Muse|           Starlight|        3060|\n",
      "|        Arcade Fire|    Rebellion (Lies)|        3048|\n",
      "|     Britney Spears|          Gimme More|        3004|\n",
      "|        The Killers| When You Were Young|        2998|\n",
      "|           Interpol|                Evil|        2989|\n",
      "|         Kanye West|       Love Lockdown|        2950|\n",
      "|     Massive Attack|            Teardrop|        2948|\n",
      "|Death Cab For Cutie|I Will Follow You...|        2947|\n",
      "|               Muse| Time Is Running Out|        2945|\n",
      "|         Bloc Party|             Banquet|        2906|\n",
      "|        Arcade Fire|Neighborhood #1 (...|        2826|\n",
      "|          Radiohead|          All I Need|        2696|\n",
      "| The Postal Service|      Nothing Better|        2670|\n",
      "|        Snow Patrol|        Chasing Cars|        2667|\n",
      "+-------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"songs_table\")  # temporary view for Spark SQL\n",
    "\n",
    "top_100_songs = spark.sql(\"\"\"\n",
    "    SELECT artist_name, track_name, COUNT(*) as times_played\n",
    "    FROM songs_table\n",
    "    GROUP BY artist_name, track_name\n",
    "    ORDER BY times_played DESC\n",
    "    LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "top_100_songs.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:28:47.312554Z",
     "end_time": "2023-08-02T13:29:11.501518Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# If you prefer the DataFrame API then:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+\n",
      "|        artist_name|          track_name|count|\n",
      "+-------------------+--------------------+-----+\n",
      "| The Postal Service|  Such Great Heights| 3992|\n",
      "|       Boy Division|Love Will Tear Us...| 3663|\n",
      "|          Radiohead|        Karma Police| 3534|\n",
      "|               Muse|Supermassive Blac...| 3483|\n",
      "|Death Cab For Cutie|     Soul Meets Body| 3479|\n",
      "|          The Knife|          Heartbeats| 3156|\n",
      "|               Muse|           Starlight| 3060|\n",
      "|        Arcade Fire|    Rebellion (Lies)| 3048|\n",
      "|     Britney Spears|          Gimme More| 3004|\n",
      "|        The Killers| When You Were Young| 2998|\n",
      "|           Interpol|                Evil| 2989|\n",
      "|         Kanye West|       Love Lockdown| 2950|\n",
      "|     Massive Attack|            Teardrop| 2948|\n",
      "|Death Cab For Cutie|I Will Follow You...| 2947|\n",
      "|               Muse| Time Is Running Out| 2945|\n",
      "|         Bloc Party|             Banquet| 2906|\n",
      "|        Arcade Fire|Neighborhood #1 (...| 2826|\n",
      "|          Radiohead|          All I Need| 2696|\n",
      "| The Postal Service|      Nothing Better| 2670|\n",
      "|        Snow Patrol|        Chasing Cars| 2667|\n",
      "+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "songs_popularity = df.groupBy('artist_name', 'track_name').count()\n",
    "top_100_songs_dataframe_api = songs_popularity.orderBy(desc('count')).limit(100)\n",
    "top_100_songs_dataframe_api.show()\n",
    "\n",
    "top_100_songs_dataframe_api.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"100_most_popular_songs.tsv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:29:11.518886Z",
     "end_time": "2023-08-02T13:30:01.372781Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part C:\n",
    "Say we define a user’s “session” of Last.fm usage to be composed of one or more\n",
    "songs played by that user, where each song is started within 20 minutes of the previous\n",
    "song’s start time.\n",
    "Create a list of the top 10 longest sessions, with the following information about each\n",
    "session:\n",
    "● userid\n",
    "● timestamp of first and last songs in the session\n",
    "● List of songs played in the session (in order of play)\n",
    "\n",
    "Approach: First say we talk on specific user (GROUP BY/Partition BY user), If I have for every song the time the previous song (ORDER BY time) played, and it's lower than 20 min than they both part of the same session, to do this we can use LAG with offset of 1 to create \"previous_song_playtime\". using \"previous_song_playtime\" it's very easy to create \"time_from_prev_song\". then the challenge is to \"number\" the session, it's easy to detect each new session -> if time from prev_song>20 min, for each new session we will create a flag col \"new session\", if we count those flags until each record (window function sum, similar to the way rank works, but with sum) we will get session_number col. Now in order find the longest one - we need to find the start/end time - smallest and highest timestamps of the session, calculate the diff -> session_length, we already have the user_id, session number and now the session_length, so all that remains is to fetch all the songs in this list. then get the top 10 of the session length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+--------------------+--------------------+\n",
      "|    user_id|         start_time|           end_time|session_duration_sec|    songs_in_session|\n",
      "+-----------+-------------------+-------------------+--------------------+--------------------+\n",
      "|user_000949|2006-02-12 19:49:31|2006-02-27 13:29:37|             1273206|[{2006-02-12 19:4...|\n",
      "|user_000997|2007-04-26 03:36:02|2007-05-10 20:55:03|             1271941|[{2007-04-26 03:3...|\n",
      "|user_000949|2007-05-01 05:41:15|2007-05-14 03:05:52|             1113877|[{2007-05-01 05:4...|\n",
      "|user_000544|2007-02-12 15:03:52|2007-02-23 02:51:08|              906436|[{2007-02-12 15:0...|\n",
      "|user_000949|2005-12-09 10:26:38|2005-12-18 06:40:04|              764006|[{2005-12-09 10:2...|\n",
      "|user_000949|2005-11-11 05:30:37|2005-11-19 00:50:07|              674370|[{2005-11-11 05:3...|\n",
      "|user_000949|2006-03-19 01:04:14|2006-03-26 20:13:45|              673771|[{2006-03-19 01:0...|\n",
      "|user_000544|2007-01-06 03:07:04|2007-01-13 15:57:45|              651041|[{2007-01-06 03:0...|\n",
      "|user_000250|2008-02-21 17:31:45|2008-02-28 23:18:03|              625578|[{2008-02-21 17:3...|\n",
      "|user_000949|2006-02-27 19:47:28|2006-03-06 21:52:35|              612307|[{2006-02-27 19:4...|\n",
      "+-----------+-------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "time_considered_new_session = 20 * 60  # 20 minutes in seconds\n",
    "\n",
    "# Calculate time_from_prev_song\n",
    "user_window = Window.partitionBy('user_id').orderBy('timestamp')\n",
    "df = df.withColumn('sec_from_prev_song', (F.col('timestamp') - F.lag('timestamp', 1).over(user_window)).cast('long'))\n",
    "# print(df.columns)\n",
    "# df.select(\"user_id\", \"timestamp\", \"sec_from_prev_song\").show(10)  # health check\n",
    "\n",
    "\n",
    "# Number the sessions, Logic: each time we find more than 'session_window' diff we raise a flag,\n",
    "# session_num is the counter of all raised flags so far == current session num\n",
    "session_flag = F.when(col('sec_from_prev_song').isNull() | (col('sec_from_prev_song') >= time_considered_new_session), lit(1)).otherwise(lit(0))\n",
    "df = df.withColumn('session_num', F.sum(session_flag).over(user_window))\n",
    "\n",
    "# print(df.columns)\n",
    "# df.select(\"user_id\", \"timestamp\", \"sec_from_prev_song\", \"user_session_num\").show(200)  # health check\n",
    "\n",
    "# For each session - calculations in order to extract session_duration_sec, and songs_in_session\n",
    "sessions = df.groupBy('user_id', 'session_num').agg(\n",
    "    F.min('timestamp').alias('start_time'),\n",
    "    F.max('timestamp').alias('end_time'),\n",
    "    F.sort_array(F.collect_list(F.struct('timestamp', 'artist_name', 'track_name'))).alias('songs_in_session')  # sort_array to maintain order\n",
    ")\n",
    "\n",
    "# spark doesn't support creating session_duration_sec in the above declaration, therefore added here\n",
    "sessions = sessions.withColumn('session_duration_sec', F.col('end_time').cast('long') - F.col('start_time').cast('long'))\n",
    "# Note: if the fact that songs_in_session starts with timestamp bother us we can use: transform(songs_in_session, x -> struct(x.artist_name, x.track_name))\n",
    "# print(sessions.columns)\n",
    "# sessions.show(40, truncate=False) # health check\n",
    "\n",
    "# Eventually get top10 by session_duration_sec\n",
    "top_10_sessions = sessions.orderBy(F.desc('session_duration_sec')).limit(10)\n",
    "top_10_sessions.select('user_id', 'start_time', 'end_time', 'session_duration_sec', 'songs_in_session').show(10)  # health check\n",
    "\n",
    "# Write to file\n",
    "top_10_sessions_to_dump = top_10_sessions.withColumn('songs_in_session', F.col('songs_in_session').cast(\"string\"))  # you can't write list to csv, convert to string\n",
    "top_10_sessions_to_dump.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"top_10_longest_sessions.tsv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:30:01.387883Z",
     "end_time": "2023-08-02T13:34:56.179890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:34:55.719151Z",
     "end_time": "2023-08-02T13:34:57.127745Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-02T13:34:57.078953Z",
     "end_time": "2023-08-02T13:34:57.130534Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
